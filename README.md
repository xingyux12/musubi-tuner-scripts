# musubi-tuner-scripts

original codebase from kohya_ss

https://github.com/kohya-ss/musubi-tuner

## ğŸ”§ Setting up the Environment

  Give unrestricted script access to powershell so venv can work:

- Open an administrator powershell window
- Type `Set-ExecutionPolicy Unrestricted` and answer A
- Close admin powershell window

## Installation

Clone the repo with `--recurse-submodules`:

```
git clone --recurse-submodules https://github.com/sdbds/musubi-tuner-scripts.git
```

# MUST USE --recurse-submodules

### Windows
Run the following PowerShell script:
```powershell
./1ã€install-uv-qinglong.ps1
```

### Linux
1. First install PowerShell:
```bash
./0ã€install pwsh.sh
```
2. Then run the installation script using PowerShell:
```powershell
sudo pwsh ./1ã€install-uv-qinglong.ps1
```
use sudo pwsh if you in Linux.

## Usage

edit 2ã€3ã€4 script before you run.

### 2ã€cache_latent_and_text_encoder.ps1
```
# Cache lantent
$dataset_config = "./toml/qinglong-datasets.toml"            # path to dataset config .toml file | æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
$vae = "./ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt" # VAE directory | VAEè·¯å¾„
$vae_dtype = ""                                              # fp16 | fp32 |bf16 default: fp16
$vae_chunk_size = 32                                         # chunk size for CausalConv3d in VAE
$vae_tiling = $True                                          # enable spatial tiling for VAE, default is False. If vae_spatial_tile_sample_min_size is set, this is automatically enabled
$vae_spatial_tile_sample_min_size = 256                      # spatial tile sample min size for VAE, default 256
$device = ""                                                 # cuda | cpu
$batch_size = ""                                             # batch size, override dataset config if dataset batch size > this
$num_workers = 0                                             # number of workers for dataset. default is cpu count-1
$skip_existing = $True                                       # skip existing cache files
$debug_mode = ""                                             # image | console
$console_width = $Host.UI.RawUI.WindowSize.Width             # console width
$console_back = "black"                                      # console background color
$console_num_images = 16                                     # number of images to show in console

# Cache text encoder
$text_encoder1 = "./ckpts/text_encoder/llava_llama3_fp16.safetensors"     # Text Encoder 1 directory | æ–‡æœ¬ç¼–ç å™¨è·¯å¾„
$text_encoder2 = "./ckpts/text_encoder_2/clip_l.safetensors"              # Text Encoder 2 directory | æ–‡æœ¬ç¼–ç å™¨è·¯å¾„
$text_encoder_batch_size = "16"                                           # batch size
$text_encoder_device = ""                                                 # cuda | cpu
$text_encoder_dtype = "bf16"                                              # fp16 | fp32 |bf16 default: fp16
$fp8_llm = $False                                                         # enable fp8 for text encoder
$text_encoder_num_workers = 0                                             # number of workers for dataset. default is cpu count-1
$text_encoder_skip_existing = $False                                       # skip existing cache files
```

### 3ã€train.ps1
```
# model_path
$dataset_config = "./toml/qinglong-datasets.toml"                                   # path to dataset config .toml file | æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
$dit = "./ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt" # DiT directory | DiTè·¯å¾„
$vae = "./ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt"                        # VAE directory | VAEè·¯å¾„
$text_encoder1 = "./ckpts/text_encoder/llava_llama3_fp16.safetensors"               # Text Encoder 1 directory | æ–‡æœ¬ç¼–ç å™¨è·¯å¾„
$text_encoder2 = "./ckpts/text_encoder_2/clip_l.safetensors"                        # Text Encoder 2 directory | æ–‡æœ¬ç¼–ç å™¨è·¯å¾„

$resume = ""                                                                        # resume from state | ä»æŸä¸ªçŠ¶æ€æ–‡ä»¶å¤¹ä¸­æ¢å¤è®­ç»ƒ
$network_weights = ""                                                               # pretrained weights for LoRA network | è‹¥éœ€è¦ä»å·²æœ‰çš„ LoRA æ¨¡å‹ä¸Šç»§ç»­è®­ç»ƒï¼Œè¯·å¡«å†™ LoRA æ¨¡å‹è·¯å¾„ã€‚

#COPY machine | å·®å¼‚ç‚¼ä¸¹æ³•
$base_weights = "" #æŒ‡å®šåˆå¹¶åˆ°åº•æ¨¡basemodelä¸­çš„æ¨¡å‹è·¯å¾„ï¼Œå¤šä¸ªç”¨ç©ºæ ¼éš”å¼€ã€‚é»˜è®¤ä¸ºç©ºï¼Œä¸ä½¿ç”¨ã€‚
$base_weights_multiplier = "1.0" #æŒ‡å®šåˆå¹¶æ¨¡å‹çš„æƒé‡ï¼Œå¤šä¸ªç”¨ç©ºæ ¼éš”å¼€ï¼Œé»˜è®¤ä¸º1.0ã€‚

#train config | è®­ç»ƒé…ç½®
$max_train_steps = ""                                                                # max train steps | æœ€å¤§è®­ç»ƒæ­¥æ•°
$max_train_epochs = 80                                                               # max train epochs | æœ€å¤§è®­ç»ƒè½®æ•°
$gradient_checkpointing = 1                                                          # æ¢¯åº¦æ£€æŸ¥ï¼Œå¼€å¯åå¯èŠ‚çº¦æ˜¾å­˜ï¼Œä½†æ˜¯é€Ÿåº¦å˜æ…¢
$gradient_accumulation_steps = 1                                                     # æ¢¯åº¦ç´¯åŠ æ•°é‡ï¼Œå˜ç›¸æ”¾å¤§batchsizeçš„å€æ•°
$guidance_scale = 1.0
$seed = 1026 # reproducable seed | è®¾ç½®è·‘æµ‹è¯•ç”¨çš„ç§å­ï¼Œè¾“å…¥ä¸€ä¸ªpromptå’Œè¿™ä¸ªç§å­å¤§æ¦‚ç‡å¾—åˆ°è®­ç»ƒå›¾ã€‚å¯ä»¥ç”¨æ¥è¯•è§¦å‘å…³é”®è¯

#timestep sampling
$timestep_sampling = "sigmoid" # æ—¶é—´æ­¥é‡‡æ ·æ–¹æ³•ï¼Œå¯é€‰ sd3ç”¨"sigma"ã€æ™®é€šDDPMç”¨"uniform" æˆ– fluxç”¨"sigmoid" æˆ–è€… "shift". shiftéœ€è¦ä¿®æ”¹discarete_flow_shiftçš„å‚æ•°
$sigmoid_scale = 1.0 # sigmoid é‡‡æ ·çš„ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º 1.0ã€‚è¾ƒå¤§çš„å€¼ä¼šä½¿é‡‡æ ·æ›´åŠ å‡åŒ€

$weighting_scheme = ""      # sigma_sqrt, logit_normal, mode, cosmap, uniform, none
$logit_mean = 0.0           # logit mean | logit å‡å€¼ é»˜è®¤0.0 åªåœ¨logit_normalä¸‹ç”Ÿæ•ˆ
$logit_std = 1.0            # logit std | logit æ ‡å‡†å·® é»˜è®¤1.0 åªåœ¨logit_normalä¸‹ç”Ÿæ•ˆ
$mode_scale = 1.29          # mode scale | mode ç¼©æ”¾ é»˜è®¤1.29 åªåœ¨modeä¸‹ç”Ÿæ•ˆ
$min_timestep = 0           #æœ€å°æ—¶åºï¼Œé»˜è®¤å€¼0
$max_timestep = 1000        #æœ€å¤§æ—¶é—´æ­¥ é»˜è®¤1000
$show_timesteps = ""        #æ˜¯å¦æ˜¾ç¤ºtimesteps

# Learning rate | å­¦ä¹ ç‡
$lr = "1e-4"
# $unet_lr = "5e-4"
# $text_encoder_lr = "2e-5"
$lr_scheduler = "cosine_with_min_lr"
# "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup" | PyTorchè‡ªå¸¦6ç§åŠ¨æ€å­¦ä¹ ç‡å‡½æ•°
# constantï¼Œå¸¸é‡ä¸å˜, constant_with_warmup çº¿æ€§å¢åŠ åä¿æŒå¸¸é‡ä¸å˜, linear çº¿æ€§å¢åŠ çº¿æ€§å‡å°‘, polynomial çº¿æ€§å¢åŠ åå¹³æ»‘è¡°å‡, cosine ä½™å¼¦æ³¢æ›²çº¿, cosine_with_restarts ä½™å¼¦æ³¢ç¡¬é‡å¯ï¼Œç¬é—´æœ€å¤§å€¼ã€‚
# æ–°å¢cosine_with_min_lr(é€‚åˆè®­ç»ƒlora)ã€warmup_stable_decay(é€‚åˆè®­ç»ƒdb)ã€inverse_sqrt
$lr_warmup_steps = 0 # warmup steps | å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼Œlr_scheduler ä¸º constant æˆ– adafactor æ—¶è¯¥å€¼éœ€è¦è®¾ä¸º0ã€‚ä»…åœ¨ lr_scheduler ä¸º constant_with_warmup æ—¶éœ€è¦å¡«å†™è¿™ä¸ªå€¼
$lr_decay_steps = 0.25 # decay steps | å­¦ä¹ ç‡è¡°å‡æ­¥æ•°ï¼Œä»…åœ¨ lr_scheduler ä¸ºwarmup_stable_decayæ—¶ éœ€è¦å¡«å†™ï¼Œä¸€èˆ¬æ˜¯10%æ€»æ­¥æ•°
$lr_scheduler_num_cycles = 1 # restarts nums | ä½™å¼¦é€€ç«é‡å¯æ¬¡æ•°ï¼Œä»…åœ¨ lr_scheduler ä¸º cosine_with_restarts æ—¶éœ€è¦å¡«å†™è¿™ä¸ªå€¼
$lr_scheduler_power = 1     #Polynomial power for polynomial scheduler |ä½™å¼¦é€€ç«power
$lr_scheduler_timescale = 0 #times scale |æ—¶é—´ç¼©æ”¾ï¼Œä»…åœ¨ lr_scheduler ä¸º inverse_sqrt æ—¶éœ€è¦å¡«å†™è¿™ä¸ªå€¼ï¼Œé»˜è®¤åŒlr_warmup_steps
$lr_scheduler_min_lr_ratio = 0.1 #min lr ratio |æœ€å°å­¦ä¹ ç‡æ¯”ç‡ï¼Œä»…åœ¨ lr_scheduler ä¸º cosine_with_min_lrã€ã€warmup_stable_decay æ—¶éœ€è¦å¡«å†™è¿™ä¸ªå€¼ï¼Œé»˜è®¤0

#network settings
$network_dim = 32 # network dim | å¸¸ç”¨ 4~128ï¼Œä¸æ˜¯è¶Šå¤§è¶Šå¥½
$network_alpha = 16 # network alpha | å¸¸ç”¨ä¸ network_dim ç›¸åŒçš„å€¼æˆ–è€…é‡‡ç”¨è¾ƒå°çš„å€¼ï¼Œå¦‚ network_dimçš„ä¸€åŠ é˜²æ­¢ä¸‹æº¢ã€‚é»˜è®¤å€¼ä¸º 1ï¼Œä½¿ç”¨è¾ƒå°çš„ alpha éœ€è¦æå‡å­¦ä¹ ç‡ã€‚
$network_dropout = 0 # network dropout | å¸¸ç”¨ 0~0.3
$dim_from_weights = $True # use dim from weights | ä»å·²æœ‰çš„ LoRA æ¨¡å‹ä¸Šç»§ç»­è®­ç»ƒæ—¶ï¼Œè‡ªåŠ¨è·å– dim
$scale_weight_norms = 0 # scale weight norms (1 is a good starting point)| scale weight norms (1 is a good starting point)

# $train_unet_only = 1 # train U-Net only | ä»…è®­ç»ƒ U-Netï¼Œå¼€å¯è¿™ä¸ªä¼šç‰ºç‰²æ•ˆæœå¤§å¹…å‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚6Gæ˜¾å­˜å¯ä»¥å¼€å¯
# $train_text_encoder_only = 0 # train Text Encoder only | ä»…è®­ç»ƒ æ–‡æœ¬ç¼–ç å™¨

#precision and accelerate/save memory
$attn_mode = "sageattn"                                                             # "flash", "sageattn", "xformers", "sdpa"
$mixed_precision = "bf16"                                                           # fp16 | fp32 |bf16 default: bf16
$dit_dtype = ""                                                                     # fp16 | fp32 |bf16 default: bf16

$vae_dtype = ""                                                                     # fp16 | fp32 |bf16 default: fp16
$vae_tiling = $True                                                                 # enable spatial tiling for VAE, default is False. If vae_spatial_tile_sample_min_size is set, this is automatically enabled
$vae_chunk_size = 32                                                                # chunk size for CausalConv3d in VAE
$vae_spatial_tile_sample_min_size = 256                                             # spatial tile sample min size for VAE, default 256

$text_encoder_dtype = ""                                                            # fp16 | fp32 |bf16 default: fp16

$fp8_base = $True                                                                   # fp8
$fp8_llm = $False                                                                   # fp8 for LLM
$max_data_loader_n_workers = 8                                                      # max data loader n workers | æœ€å¤§æ•°æ®åŠ è½½çº¿ç¨‹æ•°
$persistent_data_loader_workers = $True                                             # save every n epochs | æ¯å¤šå°‘è½®ä¿å­˜ä¸€æ¬¡

$blocks_to_swap = 0                                                                # äº¤æ¢çš„å—æ•°
$img_in_txt_in_offloading = $True                                                   # img in txt in offloading

#optimizer
$optimizer_type = "AdamW8bit"                                                       
# adamw8bit | adamw32bit | adamw16bit | adafactor | Lion | Lion8bit | 
# PagedLion8bit | AdamW | AdamW8bit | PagedAdamW8bit | AdEMAMix8bit | PagedAdEMAMix8bit
# DAdaptAdam | DAdaptLion | DAdaptAdan | DAdaptSGD | Sophia | Prodigy
$max_grad_norm = 1.0 # max grad norm | æœ€å¤§æ¢¯åº¦èŒƒæ•°ï¼Œé»˜è®¤ä¸º1.0

# wandb log
$wandb_api_key = ""                   # wandbAPI KEYï¼Œç”¨äºç™»å½•

# save and load settings | ä¿å­˜å’Œè¾“å‡ºè®¾ç½®
$output_name = "hyvideo-qinglong"  # output model name | æ¨¡å‹ä¿å­˜åç§°
$save_every_n_epochs = "10"           # save every n epochs | æ¯å¤šå°‘è½®ä¿å­˜ä¸€æ¬¡
$save_every_n_steps = ""              # save every n steps | æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡
$save_last_n_epochs = ""            # save last n epochs | ä¿å­˜æœ€åå¤šå°‘è½®
$save_last_n_steps = ""               # save last n steps | ä¿å­˜æœ€åå¤šå°‘æ­¥

# save state | ä¿å­˜è®­ç»ƒçŠ¶æ€
$save_state = $False                  # save training state | ä¿å­˜è®­ç»ƒçŠ¶æ€
$save_state_on_train_end = $False     # save state on train end |åªåœ¨è®­ç»ƒç»“æŸæœ€åä¿å­˜è®­ç»ƒçŠ¶æ€
$save_last_n_epochs_state = ""        # save last n epochs state | ä¿å­˜æœ€åå¤šå°‘è½®è®­ç»ƒçŠ¶æ€
$save_last_n_steps_state = ""         # save last n steps state | ä¿å­˜æœ€åå¤šå°‘æ­¥è®­ç»ƒçŠ¶æ€

#lycorisç»„ä»¶
$enable_lycoris = 0 # å¼€å¯lycoris
$conv_dim = 0 #å·ç§¯ dimï¼Œæ¨èï¼œ32
$conv_alpha = 0 #å·ç§¯ alphaï¼Œæ¨è1æˆ–è€…0.3
$algo = "lokr" # algoå‚æ•°ï¼ŒæŒ‡å®šè®­ç»ƒlycorisæ¨¡å‹ç§ç±»ï¼Œ
#åŒ…æ‹¬lora(å°±æ˜¯locon)ã€
#loha
#IA3
#lokr
#dylora
#full(DreamBoothå…ˆè®­ç»ƒç„¶åå¯¼å‡ºlora)
#diag-oft
#å®ƒé€šè¿‡è®­ç»ƒé€‚ç”¨äºå„å±‚è¾“å‡ºçš„æ­£äº¤å˜æ¢æ¥ä¿ç•™è¶…çƒé¢èƒ½é‡ã€‚
#æ ¹æ®åŸå§‹è®ºæ–‡ï¼Œå®ƒçš„æ”¶æ•›é€Ÿåº¦æ¯” LoRA æ›´å¿«ï¼Œä½†ä»éœ€è¿›è¡Œå®éªŒã€‚
#dim ä¸åŒºå—å¤§å°ç›¸å¯¹åº”ï¼šæˆ‘ä»¬åœ¨è¿™é‡Œå›ºå®šäº†åŒºå—å¤§å°è€Œä¸æ˜¯åŒºå—æ•°é‡ï¼Œä»¥ä½¿å…¶ä¸ LoRA æ›´å…·å¯æ¯”æ€§ã€‚

$dropout = 0 #lycorisä¸“ç”¨dropout
$preset = "attn-mlp" #é¢„è®¾è®­ç»ƒæ¨¡å—é…ç½®
#full: default preset, train all the layers in the UNet and CLIP|é»˜è®¤è®¾ç½®ï¼Œè®­ç»ƒæ‰€æœ‰Unetå’ŒClipå±‚
#full-lin: full but skip convolutional layers|è·³è¿‡å·ç§¯å±‚
#attn-mlp: train all the transformer block.|kohyaé…ç½®ï¼Œè®­ç»ƒæ‰€æœ‰transformeræ¨¡å—
#attn-onlyï¼šonly attention layer will be trained, lot of papers only do training on attn layer.|åªæœ‰æ³¨æ„åŠ›å±‚ä¼šè¢«è®­ç»ƒï¼Œå¾ˆå¤šè®ºæ–‡åªå¯¹æ³¨æ„åŠ›å±‚è¿›è¡Œè®­ç»ƒã€‚
#unet-transformer-onlyï¼š as same as kohya_ss/sd_scripts with disabled TE, or, attn-mlp preset with train_unet_only enabled.|å’Œattn-mlpç±»ä¼¼ï¼Œä½†æ˜¯å…³é—­teè®­ç»ƒ
#unet-convblock-onlyï¼š only ResBlock, UpSample, DownSample will be trained.|åªè®­ç»ƒå·ç§¯æ¨¡å—ï¼ŒåŒ…æ‹¬resã€ä¸Šä¸‹é‡‡æ ·æ¨¡å—
#./toml/example_lycoris.toml: ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨å¤–ç½®é…ç½®æ–‡ä»¶ï¼Œåˆ¶å®šå„ä¸ªå±‚å’Œæ¨¡å—ä½¿ç”¨ä¸åŒç®—æ³•è®­ç»ƒï¼Œéœ€è¦è¾“å…¥ä½ç½®æ–‡ä»¶è·¯å¾„ï¼Œå‚è€ƒæ ·ä¾‹å·²æ·»åŠ ã€‚

$factor = 8 #åªé€‚ç”¨äºlokrçš„å› å­ï¼Œ-1~8ï¼Œ8ä¸ºå…¨ç»´åº¦
$decompose_both = 0 #é€‚ç”¨äºlokrçš„å‚æ•°ï¼Œå¯¹ LoKr åˆ†è§£äº§ç”Ÿçš„ä¸¤ä¸ªçŸ©é˜µæ‰§è¡Œ LoRA åˆ†è§£ï¼ˆé»˜è®¤æƒ…å†µä¸‹åªåˆ†è§£è¾ƒå¤§çš„çŸ©é˜µï¼‰
$block_size = 4 #é€‚ç”¨äºdylora,åˆ†å‰²å—æ•°å•ä½ï¼Œæœ€å°1ä¹Ÿæœ€æ…¢ã€‚ä¸€èˆ¬4ã€8ã€12ã€16è¿™å‡ ä¸ªé€‰
$use_tucker = 0 #é€‚ç”¨äºé™¤ (IA)^3 å’Œfull
$use_scalar = 0 #æ ¹æ®ä¸åŒç®—æ³•ï¼Œè‡ªåŠ¨è°ƒæ•´åˆå§‹æƒé‡
$train_norm = 0 #å½’ä¸€åŒ–å±‚
$dora_wd = 1 #Doraæ–¹æ³•åˆ†è§£ï¼Œä½rankä½¿ç”¨ã€‚é€‚ç”¨äºLoRA, LoHa, å’ŒLoKr
$full_matrix = 0  #å…¨çŸ©é˜µåˆ†è§£
$bypass_mode = 0 #é€šé“æ¨¡å¼ï¼Œä¸“ä¸º bnb 8 ä½/4 ä½çº¿æ€§å±‚è®¾è®¡ã€‚(QLyCORIS)é€‚ç”¨äºLoRA, LoHa, å’ŒLoKr
$rescaled = 1 #é€‚ç”¨äºè®¾ç½®ç¼©æ”¾ï¼Œæ•ˆæœç­‰åŒäºOFT
$constrain = 0 #è®¾ç½®å€¼ä¸ºFLOATï¼Œæ•ˆæœç­‰åŒäºCOFT

#sample | è¾“å‡ºé‡‡æ ·å›¾ç‰‡
$enable_sample = 0 #1å¼€å¯å‡ºå›¾ï¼Œ0ç¦ç”¨
$sample_at_first = 1 #æ˜¯å¦åœ¨è®­ç»ƒå¼€å§‹æ—¶å°±å‡ºå›¾
$sample_every_n_epochs = 2 #æ¯nä¸ªepochå‡ºä¸€æ¬¡å›¾
$sample_prompts = "./toml/qinglong.txt" #promptæ–‡ä»¶è·¯å¾„

#metadata
$training_comment = "this LoRA model created by bdsqlsz'script" # training_comment | è®­ç»ƒä»‹ç»ï¼Œå¯ä»¥å†™ä½œè€…åæˆ–è€…ä½¿ç”¨è§¦å‘å…³é”®è¯
$metadata_title = "" # metadata title | å…ƒæ•°æ®æ ‡é¢˜
$metadata_author = "" # metadata author | å…ƒæ•°æ®ä½œè€…
$metadata_description = "" # metadata contact | å…ƒæ•°æ®è”ç³»æ–¹å¼
$metadata_license = "" # metadata license | å…ƒæ•°æ®è®¸å¯è¯
$metadata_tags = "" # metadata tags | å…ƒæ•°æ®æ ‡ç­¾

#huggingface settings
$async_upload = $False # push to hub | æ¨é€åˆ°huggingface
$huggingface_repo_id = "" # huggingface repo id | huggingfaceä»“åº“id
$huggingface_repo_type = "dataset" # huggingface repo type | huggingfaceä»“åº“ç±»å‹
$huggingface_path_in_repo = "" # huggingface path in repo | huggingfaceä»“åº“è·¯å¾„
$huggingface_token = "" # huggingface token | huggingfaceä»“åº“token
$huggingface_repo_visibility = "" # huggingface repo visibility | huggingfaceä»“åº“å¯è§æ€§
$save_state_to_huggingface = $False # save state to huggingface | ä¿å­˜è®­ç»ƒçŠ¶æ€åˆ°huggingface
$resume_from_huggingface = $False # resume from huggingface | ä»huggingfaceæ¢å¤è®­ç»ƒ

#DDP | å¤šå¡è®¾ç½®
$multi_gpu = 0                         #multi gpu | å¤šæ˜¾å¡è®­ç»ƒå¼€å…³ï¼Œ0å…³1å¼€ï¼Œ è¯¥å‚æ•°ä»…é™åœ¨æ˜¾å¡æ•° >= 2 ä½¿ç”¨
$highvram = 0                            #é«˜æ˜¾å­˜æ¨¡å¼ï¼Œå¼€å¯åä¼šå°½é‡ä½¿ç”¨æ˜¾å­˜
# $deepspeed = 0                         #deepspeed | ä½¿ç”¨deepspeedè®­ç»ƒï¼Œ0å…³1å¼€ï¼Œ è¯¥å‚æ•°ä»…é™åœ¨æ˜¾å¡æ•° >= 2 ä½¿ç”¨
# $zero_stage = 2                        #zero stage | zero stage 0,1,2,3,é˜¶æ®µ2ç”¨äºè®­ç»ƒ è¯¥å‚æ•°ä»…é™åœ¨æ˜¾å¡æ•° >= 2 ä½¿ç”¨
# $offload_optimizer_device = ""      #offload optimizer device | ä¼˜åŒ–å™¨æ”¾ç½®è®¾å¤‡ï¼Œcpuæˆ–è€…nvme, è¯¥å‚æ•°ä»…é™åœ¨æ˜¾å¡æ•° >= 2 ä½¿ç”¨
# $fp16_master_weights_and_gradients = 0 #fp16 master weights and gradients | fp16ä¸»æƒé‡å’Œæ¢¯åº¦ï¼Œ0å…³1å¼€ï¼Œ è¯¥å‚æ•°ä»…é™åœ¨æ˜¾å¡æ•° >= 2 ä½¿ç”¨

$ddp_timeout = 120 #ddp timeout | ddpè¶…æ—¶æ—¶é—´ï¼Œå•ä½ç§’ï¼Œ è¯¥å‚æ•°ä»…é™åœ¨æ˜¾å¡æ•° >= 2 ä½¿ç”¨
$ddp_gradient_as_bucket_view = 1 #ddp gradient as bucket view | ddpæ¢¯åº¦ä½œä¸ºæ¡¶è§†å›¾ï¼Œ0å…³1å¼€ï¼Œ è¯¥å‚æ•°ä»…é™åœ¨æ˜¾å¡æ•° >= 2 ä½¿ç”¨
$ddp_static_graph = 1 #ddp static graph | ddpé™æ€å›¾ï¼Œ0å…³1å¼€ï¼Œ è¯¥å‚æ•°ä»…é™åœ¨æ˜¾å¡æ•° >= 2 ä½¿ç”¨
```

### 4ã€convert_lora.ps1
```
$input_path="./output_dir/hyvideo-qinglong.safetensors"
$output_path="./output_dir/hyvideo-qinglong_comfy.safetensors"
$target="other" # "other" or "default"
```
