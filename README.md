# musubi-tuner-scripts

original codebase from kohya_ss

https://github.com/kohya-ss/musubi-tuner

## üîß Setting up the Environment

  Give unrestricted script access to powershell so venv can work:

- Open an administrator powershell window
- Type `Set-ExecutionPolicy Unrestricted` and answer A
- Close admin powershell window

## Installation

Clone the repo with `--recurse-submodules`:

```
git clone --recurse-submodules https://github.com/sdbds/musubi-tuner-scripts.git
```

# MUST USE --recurse-submodules

### Windows
Run the following PowerShell script:
```powershell
./1„ÄÅinstall-uv-qinglong.ps1
```

### Linux
1. First install PowerShell:
```bash
./0„ÄÅinstall pwsh.sh
```
2. Then run the installation script using PowerShell:
```powershell
sudo pwsh ./1„ÄÅinstall-uv-qinglong.ps1
```
use sudo pwsh if you in Linux.

## Usage

edit 2„ÄÅ3„ÄÅ4 script before you run.

<details>
<summary>

### 2„ÄÅcache_latent_and_text_encoder.ps1</summary>

```
# Cache lantent
$dataset_config = "./toml/qinglong-datasets.toml"            # path to dataset config .toml file | Êï∞ÊçÆÈõÜÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
$vae = "./ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt" # VAE directory | VAEË∑ØÂæÑ
$vae_dtype = ""                                              # fp16 | fp32 |bf16 default: fp16
$vae_chunk_size = 32                                         # chunk size for CausalConv3d in VAE
$vae_tiling = $True                                          # enable spatial tiling for VAE, default is False. If vae_spatial_tile_sample_min_size is set, this is automatically enabled
$vae_spatial_tile_sample_min_size = 256                      # spatial tile sample min size for VAE, default 256
$device = ""                                                 # cuda | cpu
$batch_size = ""                                             # batch size, override dataset config if dataset batch size > this
$num_workers = 0                                             # number of workers for dataset. default is cpu count-1
$skip_existing = $True                                       # skip existing cache files
$debug_mode = ""                                             # image | console
$console_width = $Host.UI.RawUI.WindowSize.Width             # console width
$console_back = "black"                                      # console background color
$console_num_images = 16                                     # number of images to show in console

# Cache text encoder
$text_encoder1 = "./ckpts/text_encoder/llava_llama3_fp16.safetensors"     # Text Encoder 1 directory | ÊñáÊú¨ÁºñÁ†ÅÂô®Ë∑ØÂæÑ
$text_encoder2 = "./ckpts/text_encoder_2/clip_l.safetensors"              # Text Encoder 2 directory | ÊñáÊú¨ÁºñÁ†ÅÂô®Ë∑ØÂæÑ
$text_encoder_batch_size = "16"                                           # batch size
$text_encoder_device = ""                                                 # cuda | cpu
$text_encoder_dtype = "bf16"                                              # fp16 | fp32 |bf16 default: fp16
$fp8_llm = $False                                                         # enable fp8 for text encoder
$text_encoder_num_workers = 0                                             # number of workers for dataset. default is cpu count-1
$text_encoder_skip_existing = $False                                       # skip existing cache files
```
</details>

<details>
<summary>

### 3„ÄÅtrain.ps1
</summary>

```
#ËÆ≠ÁªÉÊ®°Âºè(Lora„ÄÅdb)
$train_mode = "Lora"

# model_path
$dataset_config = "./toml/qinglong-datasets.toml"                                   # path to dataset config .toml file | Êï∞ÊçÆÈõÜÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
$dit = "./ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt" # DiT directory | DiTË∑ØÂæÑ
$vae = "./ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt"                        # VAE directory | VAEË∑ØÂæÑ
$text_encoder1 = "./ckpts/text_encoder/llava_llama3_fp16.safetensors"               # Text Encoder 1 directory | ÊñáÊú¨ÁºñÁ†ÅÂô®Ë∑ØÂæÑ
$text_encoder2 = "./ckpts/text_encoder_2/clip_l.safetensors"                        # Text Encoder 2 directory | ÊñáÊú¨ÁºñÁ†ÅÂô®Ë∑ØÂæÑ

$resume = ""                                                                        # resume from state | ‰ªéÊüê‰∏™Áä∂ÊÄÅÊñá‰ª∂Â§π‰∏≠ÊÅ¢Â§çËÆ≠ÁªÉ
$network_weights = ""                                                               # pretrained weights for LoRA network | Ëã•ÈúÄË¶Å‰ªéÂ∑≤ÊúâÁöÑ LoRA Ê®°Âûã‰∏äÁªßÁª≠ËÆ≠ÁªÉÔºåËØ∑Â°´ÂÜô LoRA Ê®°ÂûãË∑ØÂæÑ„ÄÇ

#COPY machine | Â∑ÆÂºÇÁÇº‰∏πÊ≥ï
$base_weights = "" #ÊåáÂÆöÂêàÂπ∂Âà∞Â∫ïÊ®°basemodel‰∏≠ÁöÑÊ®°ÂûãË∑ØÂæÑÔºåÂ§ö‰∏™Áî®Á©∫Ê†ºÈöîÂºÄ„ÄÇÈªòËÆ§‰∏∫Á©∫Ôºå‰∏ç‰ΩøÁî®„ÄÇ
$base_weights_multiplier = "1.0" #ÊåáÂÆöÂêàÂπ∂Ê®°ÂûãÁöÑÊùÉÈáçÔºåÂ§ö‰∏™Áî®Á©∫Ê†ºÈöîÂºÄÔºåÈªòËÆ§‰∏∫1.0„ÄÇ

#train config | ËÆ≠ÁªÉÈÖçÁΩÆ
$max_train_steps = ""                                                                # max train steps | ÊúÄÂ§ßËÆ≠ÁªÉÊ≠•Êï∞
$max_train_epochs = 15                                                               # max train epochs | ÊúÄÂ§ßËÆ≠ÁªÉËΩÆÊï∞
$gradient_checkpointing = 1                                                          # Ê¢ØÂ∫¶Ê£ÄÊü•ÔºåÂºÄÂêØÂêéÂèØËäÇÁ∫¶ÊòæÂ≠òÔºå‰ΩÜÊòØÈÄüÂ∫¶ÂèòÊÖ¢
$gradient_accumulation_steps = 4                                                     # Ê¢ØÂ∫¶Á¥ØÂä†Êï∞ÈáèÔºåÂèòÁõ∏ÊîæÂ§ßbatchsizeÁöÑÂÄçÊï∞
$guidance_scale = 1.0
$seed = 1026 # reproducable seed | ËÆæÁΩÆË∑ëÊµãËØïÁî®ÁöÑÁßçÂ≠êÔºåËæìÂÖ•‰∏Ä‰∏™promptÂíåËøô‰∏™ÁßçÂ≠êÂ§ßÊ¶ÇÁéáÂæóÂà∞ËÆ≠ÁªÉÂõæ„ÄÇÂèØ‰ª•Áî®Êù•ËØïËß¶ÂèëÂÖ≥ÈîÆËØç

#timestep sampling
$timestep_sampling = "sigmoid" # Êó∂Èó¥Ê≠•ÈááÊ†∑ÊñπÊ≥ïÔºåÂèØÈÄâ sd3Áî®"sigma"„ÄÅÊôÆÈÄöDDPMÁî®"uniform" Êàñ fluxÁî®"sigmoid" ÊàñËÄÖ "shift". shiftÈúÄË¶Å‰øÆÊîπdiscarete_flow_shiftÁöÑÂèÇÊï∞
$discrete_flow_shift = 1.0 # Euler Á¶ªÊï£Ë∞ÉÂ∫¶Âô®ÁöÑÁ¶ªÊï£ÊµÅ‰ΩçÁßªÔºåsd3ÈªòËÆ§‰∏∫3.0
$sigmoid_scale = 1.0 # sigmoid ÈááÊ†∑ÁöÑÁº©ÊîæÂõ†Â≠êÔºåÈªòËÆ§‰∏∫ 1.0„ÄÇËæÉÂ§ßÁöÑÂÄº‰ºö‰ΩøÈááÊ†∑Êõ¥Âä†ÂùáÂåÄ

$weighting_scheme = ""      # sigma_sqrt, logit_normal, mode, cosmap, uniform, none
$logit_mean = 0           # logit mean | logit ÂùáÂÄº ÈªòËÆ§0.0 Âè™Âú®logit_normal‰∏ãÁîüÊïà
$logit_std = 1.0            # logit std | logit Ê†áÂáÜÂ∑Æ ÈªòËÆ§1.0 Âè™Âú®logit_normal‰∏ãÁîüÊïà
$mode_scale = 1.29          # mode scale | mode Áº©Êîæ ÈªòËÆ§1.29 Âè™Âú®mode‰∏ãÁîüÊïà
$min_timestep = 0           #ÊúÄÂ∞èÊó∂Â∫èÔºåÈªòËÆ§ÂÄº0
$max_timestep = 1000        #ÊúÄÂ§ßÊó∂Èó¥Ê≠• ÈªòËÆ§1000
$show_timesteps = ""        #ÊòØÂê¶ÊòæÁ§∫timestepsÔºå console/images

# Learning rate | Â≠¶‰π†Áéá
$lr = "1e-3"
# $unet_lr = "5e-4"
# $text_encoder_lr = "2e-5"
$lr_scheduler = "cosine_with_min_lr"
# "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup" | PyTorchËá™Â∏¶6ÁßçÂä®ÊÄÅÂ≠¶‰π†ÁéáÂáΩÊï∞
# constantÔºåÂ∏∏Èáè‰∏çÂèò, constant_with_warmup Á∫øÊÄßÂ¢ûÂä†Âêé‰øùÊåÅÂ∏∏Èáè‰∏çÂèò, linear Á∫øÊÄßÂ¢ûÂä†Á∫øÊÄßÂáèÂ∞ë, polynomial Á∫øÊÄßÂ¢ûÂä†ÂêéÂπ≥ÊªëË°∞Âáè, cosine ‰ΩôÂº¶Ê≥¢Êõ≤Á∫ø, cosine_with_restarts ‰ΩôÂº¶Ê≥¢Á°¨ÈáçÂêØÔºåÁû¨Èó¥ÊúÄÂ§ßÂÄº„ÄÇ
# Êñ∞Â¢ûcosine_with_min_lr(ÈÄÇÂêàËÆ≠ÁªÉlora)„ÄÅwarmup_stable_decay(ÈÄÇÂêàËÆ≠ÁªÉdb)„ÄÅinverse_sqrt
$lr_warmup_steps = 0 # warmup steps | Â≠¶‰π†ÁéáÈ¢ÑÁÉ≠Ê≠•Êï∞Ôºålr_scheduler ‰∏∫ constant Êàñ adafactor Êó∂ËØ•ÂÄºÈúÄË¶ÅËÆæ‰∏∫0„ÄÇ‰ªÖÂú® lr_scheduler ‰∏∫ constant_with_warmup Êó∂ÈúÄË¶ÅÂ°´ÂÜôËøô‰∏™ÂÄº
$lr_decay_steps = 0.25 # decay steps | Â≠¶‰π†ÁéáË°∞ÂáèÊ≠•Êï∞Ôºå‰ªÖÂú® lr_scheduler ‰∏∫warmup_stable_decayÊó∂ ÈúÄË¶ÅÂ°´ÂÜôÔºå‰∏ÄËà¨ÊòØ10%ÊÄªÊ≠•Êï∞
$lr_scheduler_num_cycles = 1 # restarts nums | ‰ΩôÂº¶ÈÄÄÁÅ´ÈáçÂêØÊ¨°Êï∞Ôºå‰ªÖÂú® lr_scheduler ‰∏∫ cosine_with_restarts Êó∂ÈúÄË¶ÅÂ°´ÂÜôËøô‰∏™ÂÄº
$lr_scheduler_power = 1     #Polynomial power for polynomial scheduler |‰ΩôÂº¶ÈÄÄÁÅ´power
$lr_scheduler_timescale = 0 #times scale |Êó∂Èó¥Áº©ÊîæÔºå‰ªÖÂú® lr_scheduler ‰∏∫ inverse_sqrt Êó∂ÈúÄË¶ÅÂ°´ÂÜôËøô‰∏™ÂÄºÔºåÈªòËÆ§Âêålr_warmup_steps
$lr_scheduler_min_lr_ratio = 0.1 #min lr ratio |ÊúÄÂ∞èÂ≠¶‰π†ÁéáÊØîÁéáÔºå‰ªÖÂú® lr_scheduler ‰∏∫ cosine_with_min_lr„ÄÅ„ÄÅwarmup_stable_decay Êó∂ÈúÄË¶ÅÂ°´ÂÜôËøô‰∏™ÂÄºÔºåÈªòËÆ§0

#network settings
$network_dim = 32 # network dim | Â∏∏Áî® 4~128Ôºå‰∏çÊòØË∂äÂ§ßË∂äÂ•Ω
$network_alpha = 16 # network alpha | Â∏∏Áî®‰∏é network_dim Áõ∏ÂêåÁöÑÂÄºÊàñËÄÖÈááÁî®ËæÉÂ∞èÁöÑÂÄºÔºåÂ¶Ç network_dimÁöÑ‰∏ÄÂçä Èò≤Ê≠¢‰∏ãÊ∫¢„ÄÇÈªòËÆ§ÂÄº‰∏∫ 1Ôºå‰ΩøÁî®ËæÉÂ∞èÁöÑ alpha ÈúÄË¶ÅÊèêÂçáÂ≠¶‰π†Áéá„ÄÇ
$network_dropout = 0 # network dropout | Â∏∏Áî® 0~0.3
$dim_from_weights = $True # use dim from weights | ‰ªéÂ∑≤ÊúâÁöÑ LoRA Ê®°Âûã‰∏äÁªßÁª≠ËÆ≠ÁªÉÊó∂ÔºåËá™Âä®Ëé∑Âèñ dim
$scale_weight_norms = 0 # scale weight norms (1 is a good starting point)| scale weight norms (1 is a good starting point)

# $train_unet_only = 1 # train U-Net only | ‰ªÖËÆ≠ÁªÉ U-NetÔºåÂºÄÂêØËøô‰∏™‰ºöÁâ∫Áâ≤ÊïàÊûúÂ§ßÂπÖÂáèÂ∞ëÊòæÂ≠ò‰ΩøÁî®„ÄÇ6GÊòæÂ≠òÂèØ‰ª•ÂºÄÂêØ
# $train_text_encoder_only = 0 # train Text Encoder only | ‰ªÖËÆ≠ÁªÉ ÊñáÊú¨ÁºñÁ†ÅÂô®

#precision and accelerate/save memory
$attn_mode = "xformers"                                                                # "flash", "sageattn", "xformers", "sdpa"
$split_attn = $True                                                                 # split attention | split attention
$mixed_precision = "bf16"                                                           # fp16 |bf16 default: bf16
# $full_fp16 = $False
# $full_bf16 = $True
$dit_dtype = ""                                                                     # fp16 | fp32 |bf16 default: bf16

$vae_dtype = ""                                                                     # fp16 | fp32 |bf16 default: fp16
$vae_tiling = $True                                                                 # enable spatial tiling for VAE, default is False. If vae_spatial_tile_sample_min_size is set, this is automatically enabled
$vae_chunk_size = 32                                                                # chunk size for CausalConv3d in VAE
$vae_spatial_tile_sample_min_size = 256                                             # spatial tile sample min size for VAE, default 256

$text_encoder_dtype = ""                                                            # fp16 | fp32 |bf16 default: fp16

$fp8_base = $True                                                                   # fp8
$fp8_llm = $False                                                                   # fp8 for LLM
$max_data_loader_n_workers = 8                                                      # max data loader n workers | ÊúÄÂ§ßÊï∞ÊçÆÂä†ËΩΩÁ∫øÁ®ãÊï∞
$persistent_data_loader_workers = $True                                             # save every n epochs | ÊØèÂ§öÂ∞ëËΩÆ‰øùÂ≠ò‰∏ÄÊ¨°

$blocks_to_swap = 0                                                                 # ‰∫§Êç¢ÁöÑÂùóÊï∞
$img_in_txt_in_offloading = $True                                                   # img in txt in offloading

#optimizer
$optimizer_type = "AdamW8bit"                                                       
# adamw8bit | adamw32bit | adamw16bit | adafactor | Lion | Lion8bit | 
# PagedLion8bit | AdamW | AdamW8bit | PagedAdamW8bit | AdEMAMix8bit | PagedAdEMAMix8bit
# DAdaptAdam | DAdaptLion | DAdaptAdan | DAdaptSGD | Sophia | Prodigy
$max_grad_norm = 1.0 # max grad norm | ÊúÄÂ§ßÊ¢ØÂ∫¶ËåÉÊï∞ÔºåÈªòËÆ§‰∏∫1.0

# wandb log
$wandb_api_key = ""                   # wandbAPI KEYÔºåÁî®‰∫éÁôªÂΩï

# save and load settings | ‰øùÂ≠òÂíåËæìÂá∫ËÆæÁΩÆ
$output_name = "hyvideo-qinglong"  # output model name | Ê®°Âûã‰øùÂ≠òÂêçÁß∞
$save_every_n_epochs = "10"           # save every n epochs | ÊØèÂ§öÂ∞ëËΩÆ‰øùÂ≠ò‰∏ÄÊ¨°
$save_every_n_steps = ""              # save every n steps | ÊØèÂ§öÂ∞ëÊ≠•‰øùÂ≠ò‰∏ÄÊ¨°
$save_last_n_epochs = ""            # save last n epochs | ‰øùÂ≠òÊúÄÂêéÂ§öÂ∞ëËΩÆ
$save_last_n_steps = ""               # save last n steps | ‰øùÂ≠òÊúÄÂêéÂ§öÂ∞ëÊ≠•

# save state | ‰øùÂ≠òËÆ≠ÁªÉÁä∂ÊÄÅ
$save_state = $False                  # save training state | ‰øùÂ≠òËÆ≠ÁªÉÁä∂ÊÄÅ
$save_state_on_train_end = $False     # save state on train end |Âè™Âú®ËÆ≠ÁªÉÁªìÊùüÊúÄÂêé‰øùÂ≠òËÆ≠ÁªÉÁä∂ÊÄÅ
$save_last_n_epochs_state = ""        # save last n epochs state | ‰øùÂ≠òÊúÄÂêéÂ§öÂ∞ëËΩÆËÆ≠ÁªÉÁä∂ÊÄÅ
$save_last_n_steps_state = ""         # save last n steps state | ‰øùÂ≠òÊúÄÂêéÂ§öÂ∞ëÊ≠•ËÆ≠ÁªÉÁä∂ÊÄÅ

#LORA_PLUS
$enable_lora_plus = $True
$loraplus_lr_ratio = 4                #recommend 4~16

#target blocks
$enable_blocks = $False
$enable_double_blocks_only = $False
$exclude_patterns="" # Specify the values as a list. For example, "exclude_patterns=[r'.*single_blocks.*', r'.*double_blocks\.[0-9]\..*']".
$include_patterns="" # Specify the values as a list. For example, "include_patterns=[r'.*single_blocks\.\d{2}\.linear.*']".

#lycorisÁªÑ‰ª∂
$enable_lycoris = $False # ÂºÄÂêØlycoris
$conv_dim = 0 #Âç∑ÁßØ dimÔºåÊé®ËçêÔºú32
$conv_alpha = 0 #Âç∑ÁßØ alphaÔºåÊé®Ëçê1ÊàñËÄÖ0.3
$algo = "lokr" # algoÂèÇÊï∞ÔºåÊåáÂÆöËÆ≠ÁªÉlycorisÊ®°ÂûãÁßçÁ±ªÔºå
#ÂåÖÊã¨lora(Â∞±ÊòØlocon)„ÄÅ
#loha
#IA3
#lokr
#dylora
#full(DreamBoothÂÖàËÆ≠ÁªÉÁÑ∂ÂêéÂØºÂá∫lora)
#diag-oft
#ÂÆÉÈÄöËøáËÆ≠ÁªÉÈÄÇÁî®‰∫éÂêÑÂ±ÇËæìÂá∫ÁöÑÊ≠£‰∫§ÂèòÊç¢Êù•‰øùÁïôË∂ÖÁêÉÈù¢ËÉΩÈáè„ÄÇ
#Ê†πÊçÆÂéüÂßãËÆ∫ÊñáÔºåÂÆÉÁöÑÊî∂ÊïõÈÄüÂ∫¶ÊØî LoRA Êõ¥Âø´Ôºå‰ΩÜ‰ªçÈúÄËøõË°åÂÆûÈ™å„ÄÇ
#dim ‰∏éÂå∫ÂùóÂ§ßÂ∞èÁõ∏ÂØπÂ∫îÔºöÊàë‰ª¨Âú®ËøôÈáåÂõ∫ÂÆö‰∫ÜÂå∫ÂùóÂ§ßÂ∞èËÄå‰∏çÊòØÂå∫ÂùóÊï∞ÈáèÔºå‰ª•‰ΩøÂÖ∂‰∏é LoRA Êõ¥ÂÖ∑ÂèØÊØîÊÄß„ÄÇ

$dropout = 0 #lycoris‰∏ìÁî®dropout
$preset = "attn-mlp" #È¢ÑËÆæËÆ≠ÁªÉÊ®°ÂùóÈÖçÁΩÆ
#full: default preset, train all the layers in the UNet and CLIP|ÈªòËÆ§ËÆæÁΩÆÔºåËÆ≠ÁªÉÊâÄÊúâUnetÂíåClipÂ±Ç
#full-lin: full but skip convolutional layers|Ë∑≥ËøáÂç∑ÁßØÂ±Ç
#attn-mlp: train all the transformer block.|kohyaÈÖçÁΩÆÔºåËÆ≠ÁªÉÊâÄÊúâtransformerÊ®°Âùó
#attn-onlyÔºöonly attention layer will be trained, lot of papers only do training on attn layer.|Âè™ÊúâÊ≥®ÊÑèÂäõÂ±Ç‰ºöË¢´ËÆ≠ÁªÉÔºåÂæàÂ§öËÆ∫ÊñáÂè™ÂØπÊ≥®ÊÑèÂäõÂ±ÇËøõË°åËÆ≠ÁªÉ„ÄÇ
#unet-transformer-onlyÔºö as same as kohya_ss/sd_scripts with disabled TE, or, attn-mlp preset with train_unet_only enabled.|Âíåattn-mlpÁ±ª‰ººÔºå‰ΩÜÊòØÂÖ≥Èó≠teËÆ≠ÁªÉ
#unet-convblock-onlyÔºö only ResBlock, UpSample, DownSample will be trained.|Âè™ËÆ≠ÁªÉÂç∑ÁßØÊ®°ÂùóÔºåÂåÖÊã¨res„ÄÅ‰∏ä‰∏ãÈááÊ†∑Ê®°Âùó
#./toml/example_lycoris.toml: ‰πüÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®Â§ñÁΩÆÈÖçÁΩÆÊñá‰ª∂ÔºåÂà∂ÂÆöÂêÑ‰∏™Â±ÇÂíåÊ®°Âùó‰ΩøÁî®‰∏çÂêåÁÆóÊ≥ïËÆ≠ÁªÉÔºåÈúÄË¶ÅËæìÂÖ•‰ΩçÁΩÆÊñá‰ª∂Ë∑ØÂæÑÔºåÂèÇËÄÉÊ†∑‰æãÂ∑≤Ê∑ªÂä†„ÄÇ

$factor = 8 #Âè™ÈÄÇÁî®‰∫élokrÁöÑÂõ†Â≠êÔºå-1~8Ôºå8‰∏∫ÂÖ®Áª¥Â∫¶
$decompose_both = $false #ÈÄÇÁî®‰∫élokrÁöÑÂèÇÊï∞ÔºåÂØπ LoKr ÂàÜËß£‰∫ßÁîüÁöÑ‰∏§‰∏™Áü©ÈòµÊâßË°å LoRA ÂàÜËß£ÔºàÈªòËÆ§ÊÉÖÂÜµ‰∏ãÂè™ÂàÜËß£ËæÉÂ§ßÁöÑÁü©ÈòµÔºâ
$block_size = 4 #ÈÄÇÁî®‰∫édylora,ÂàÜÂâ≤ÂùóÊï∞Âçï‰ΩçÔºåÊúÄÂ∞è1‰πüÊúÄÊÖ¢„ÄÇ‰∏ÄËà¨4„ÄÅ8„ÄÅ12„ÄÅ16ËøôÂá†‰∏™ÈÄâ
$use_tucker = $false #ÈÄÇÁî®‰∫éÈô§ (IA)^3 Âíåfull
$use_scalar = $false #Ê†πÊçÆ‰∏çÂêåÁÆóÊ≥ïÔºåËá™Âä®Ë∞ÉÊï¥ÂàùÂßãÊùÉÈáç
$train_norm = $false #ÂΩí‰∏ÄÂåñÂ±Ç
$dora_wd = 1 #DoraÊñπÊ≥ïÂàÜËß£Ôºå‰Ωérank‰ΩøÁî®„ÄÇÈÄÇÁî®‰∫éLoRA, LoHa, ÂíåLoKr
$full_matrix = $false  #ÂÖ®Áü©ÈòµÂàÜËß£
$bypass_mode = $false #ÈÄöÈÅìÊ®°ÂºèÔºå‰∏ì‰∏∫ bnb 8 ‰Ωç/4 ‰ΩçÁ∫øÊÄßÂ±ÇËÆæËÆ°„ÄÇ(QLyCORIS)ÈÄÇÁî®‰∫éLoRA, LoHa, ÂíåLoKr
$rescaled = 1 #ÈÄÇÁî®‰∫éËÆæÁΩÆÁº©ÊîæÔºåÊïàÊûúÁ≠âÂêå‰∫éOFT
$constrain = $false #ËÆæÁΩÆÂÄº‰∏∫FLOATÔºåÊïàÊûúÁ≠âÂêå‰∫éCOFT

#sample | ËæìÂá∫ÈááÊ†∑ÂõæÁâá
$enable_sample = $True #1ÂºÄÂêØÂá∫ÂõæÔºå0Á¶ÅÁî®
$sample_at_first = 1 #ÊòØÂê¶Âú®ËÆ≠ÁªÉÂºÄÂßãÊó∂Â∞±Âá∫Âõæ
$sample_every_n_epochs = 2 #ÊØèn‰∏™epochÂá∫‰∏ÄÊ¨°Âõæ
$sample_prompts = "./toml/qinglong.txt" #promptÊñá‰ª∂Ë∑ØÂæÑ

#metadata
$training_comment = "this LoRA model created by bdsqlsz'script" # training_comment | ËÆ≠ÁªÉ‰ªãÁªçÔºåÂèØ‰ª•ÂÜô‰ΩúËÄÖÂêçÊàñËÄÖ‰ΩøÁî®Ëß¶ÂèëÂÖ≥ÈîÆËØç
$metadata_title = "" # metadata title | ÂÖÉÊï∞ÊçÆÊ†áÈ¢ò
$metadata_author = "" # metadata author | ÂÖÉÊï∞ÊçÆ‰ΩúËÄÖ
$metadata_description = "" # metadata contact | ÂÖÉÊï∞ÊçÆËÅîÁ≥ªÊñπÂºè
$metadata_license = "" # metadata license | ÂÖÉÊï∞ÊçÆËÆ∏ÂèØËØÅ
$metadata_tags = "" # metadata tags | ÂÖÉÊï∞ÊçÆÊ†áÁ≠æ

#huggingface settings
$async_upload = $False # push to hub | Êé®ÈÄÅÂà∞huggingface
$huggingface_repo_id = "" # huggingface repo id | huggingface‰ªìÂ∫ìid
$huggingface_repo_type = "dataset" # huggingface repo type | huggingface‰ªìÂ∫ìÁ±ªÂûã
$huggingface_path_in_repo = "" # huggingface path in repo | huggingface‰ªìÂ∫ìË∑ØÂæÑ
$huggingface_token = "" # huggingface token | huggingface‰ªìÂ∫ìtoken
$huggingface_repo_visibility = "" # huggingface repo visibility | huggingface‰ªìÂ∫ìÂèØËßÅÊÄß
$save_state_to_huggingface = $False # save state to huggingface | ‰øùÂ≠òËÆ≠ÁªÉÁä∂ÊÄÅÂà∞huggingface
$resume_from_huggingface = $False # resume from huggingface | ‰ªéhuggingfaceÊÅ¢Â§çËÆ≠ÁªÉ

#DDP | Â§öÂç°ËÆæÁΩÆ
$multi_gpu = $False                         #multi gpu | Â§öÊòæÂç°ËÆ≠ÁªÉÂºÄÂÖ≥Ôºå0ÂÖ≥1ÂºÄÔºå ËØ•ÂèÇÊï∞‰ªÖÈôêÂú®ÊòæÂç°Êï∞ >= 2 ‰ΩøÁî®
# $highvram = 0                            #È´òÊòæÂ≠òÊ®°ÂºèÔºåÂºÄÂêØÂêé‰ºöÂ∞ΩÈáè‰ΩøÁî®ÊòæÂ≠ò
# $deepspeed = 0                         #deepspeed | ‰ΩøÁî®deepspeedËÆ≠ÁªÉÔºå0ÂÖ≥1ÂºÄÔºå ËØ•ÂèÇÊï∞‰ªÖÈôêÂú®ÊòæÂç°Êï∞ >= 2 ‰ΩøÁî®
# $zero_stage = 2                        #zero stage | zero stage 0,1,2,3,Èò∂ÊÆµ2Áî®‰∫éËÆ≠ÁªÉ ËØ•ÂèÇÊï∞‰ªÖÈôêÂú®ÊòæÂç°Êï∞ >= 2 ‰ΩøÁî®
# $offload_optimizer_device = ""      #offload optimizer device | ‰ºòÂåñÂô®ÊîæÁΩÆËÆæÂ§áÔºåcpuÊàñËÄÖnvme, ËØ•ÂèÇÊï∞‰ªÖÈôêÂú®ÊòæÂç°Êï∞ >= 2 ‰ΩøÁî®
# $fp16_master_weights_and_gradients = 0 #fp16 master weights and gradients | fp16‰∏ªÊùÉÈáçÂíåÊ¢ØÂ∫¶Ôºå0ÂÖ≥1ÂºÄÔºå ËØ•ÂèÇÊï∞‰ªÖÈôêÂú®ÊòæÂç°Êï∞ >= 2 ‰ΩøÁî®

$ddp_timeout = 120 #ddp timeout | ddpË∂ÖÊó∂Êó∂Èó¥ÔºåÂçï‰ΩçÁßíÔºå ËØ•ÂèÇÊï∞‰ªÖÈôêÂú®ÊòæÂç°Êï∞ >= 2 ‰ΩøÁî®
$ddp_gradient_as_bucket_view = 1 #ddp gradient as bucket view | ddpÊ¢ØÂ∫¶‰Ωú‰∏∫Ê°∂ËßÜÂõæÔºå0ÂÖ≥1ÂºÄÔºå ËØ•ÂèÇÊï∞‰ªÖÈôêÂú®ÊòæÂç°Êï∞ >= 2 ‰ΩøÁî®
$ddp_static_graph = 1 #ddp static graph | ddpÈùôÊÄÅÂõæÔºå0ÂÖ≥1ÂºÄÔºå ËØ•ÂèÇÊï∞‰ªÖÈôêÂú®ÊòæÂç°Êï∞ >= 2 ‰ΩøÁî®
```
</details>

<details>
<summary>

### 4„ÄÅconvert_lora.ps1
</summary>

```
$input_path="./output_dir/hyvideo-qinglong.safetensors"
$output_path="./output_dir/hyvideo-qinglong_comfy.safetensors"
$target="other" # "other" or "default"
```

</details>

<details>
<summary>

### 5„ÄÅgenerate.ps1
</summary>

```
#Parameters from hv_generate_video.py
$dit = "./ckpts/hunyuan-video-t2v-720p/transformers/mp_rank_00_model_states.pt" # DiT checkpoint path or directory
$vae = "./ckpts/hunyuan-video-t2v-720p/vae/pytorch_model.pt" # VAE checkpoint path or directory
$vae_dtype = "" # data type for VAE, default is float16
$text_encoder1 = "./ckpts/text_encoder/llava_llama3_fp16.safetensors" # Text Encoder 1 directory
$text_encoder2 = "./ckpts/text_encoder_2/clip_l.safetensors" # Text Encoder 2 directory

# LoRA
$lora_weight = "./output_dir/hyvideo-qinglong.safetensors" # LoRA weight path
$lora_multiplier = "1.0" # LoRA multiplier

$prompt = """ a girl with long, flowing green hair adorned with a hair
ornament, a yellow flower, and a yellow rose. Her hair falls between her
eyes, and she has heterochromia, with one eye being blue and the other brown
or yellow. She is looking directly at the viewer with her mouth slightly
open, then laughting. Her attire consists of a green crop top
with puffy short sleeves, which are detached, revealing her collarbone and
bare shoulders. The top is complemented by a green skirt, and she wears a
green choker around her neck. Adding to her unique appearance, she has deer
ears and reindeer antlers, and a mini crown rests atop her head. A brooch and
a green bow further accentuate her outfit. The background is simple and
black, ensuring that the focus remains solely on the a girl.
"""
$video_size = "512 512" # video size
$video_length = 129 # video length
$infer_steps = 50 # number of inference steps
$save_path = "./output_dir" # path to save generated video
$seed = 1026 # Seed for evaluation.
$embedded_cfg_scale = 6.0 # Embeded classifier free guidance scale.

# Flow Matching
$flow_shift = 7.0 # Shift factor for flow matching schedulers.

$fp8 = $true # use fp8 for DiT model
$fp8_llm = $false # use fp8 for Text Encoder 1 (LLM)
$device = "" # device to use for inference. If None, use CUDA if available, otherwise use CPU
$attn_mode = "sageattn" # attention mode
$split_attn = $true # use split attention
$vae_chunk_size = 32 # chunk size for CausalConv3d in VAE
$vae_spatial_tile_sample_min_size = 128 # spatial tile sample min size for VAE, default 256
$blocks_to_swap = 0 # number of blocks to swap in the model
$img_in_txt_in_offloading = $true # offload img_in and txt_in to cpu
$output_type = "video" # output type
$no_metadata = $false # do not save metadata
$latent_path = "" # path to latent for decode. no inference
```
</details>